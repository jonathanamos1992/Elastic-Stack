# Exercise

Step 1 - Start from Alerts

We'll start with Security Onion

Started with data:
<img width="736" height="92" alt="image" src="https://github.com/user-attachments/assets/d5fcc2ac-c188-4334-baa8-9501ad1a67ad" />

See the alerts

<img width="2883" height="444" alt="image" src="https://github.com/user-attachments/assets/e02091d3-41e8-462e-a975-75b7fd0936ae" />

Goal right now is not to hunt domians or URLs-just identify the internal host that keeps appearing across alerts.

Saw the alert fired 48 times for "ET HUNTING GENERIC SUSPICIOUS POST to Dotted Quad with Fake Browser 1" so did a quick drilldown and saw the source IP of 172.17.0.99 making a POSt request to destination IP of 79.124.78.197 over port 80

<img width="2840" height="545" alt="image" src="https://github.com/user-attachments/assets/464ba1b1-5c7f-4362-a61d-0d7830a48539" />

<img width="1908" height="269" alt="image" src="https://github.com/user-attachments/assets/de433a24-d480-4e6d-9cd8-73f7766f417d" />

From ChatGPT:

<img width="652" height="147" alt="image" src="https://github.com/user-attachments/assets/8fc49e04-a6fd-41f4-8992-d72072e0eb18" />

When we see alerts, this one generated by Suricata for instance, we are seeing the alert record and not the full HTTP transaction.
These often don't include fields like http.request.method or network protocol because they're just detection metadata.

Drilldown

<img width="2011" height="94" alt="image" src="https://github.com/user-attachments/assets/e873d5e3-7d09-4551-8db2-b1c45a9b832e" />

We still didn't see the fields we wanted to we are switching to Kibana
Reason being alerts don't always contain parsed HTTP fields.

ChatGPT wants us to look for these fields
<img width="285" height="91" alt="image" src="https://github.com/user-attachments/assets/518c69f2-62ac-4b57-b480-b5966913e5f6" />

### http.request.method
#### We use the method to understand what role the host is playing in the conversation.

Malware talks differently than normal users.
For example:
GET= usually "give me a file/page"
POST= usually "I'm sending data back"

So when we see repeated POSTs from one internal host, that often suggests check-ins, exfiltration, or C2 communication=not normal browsing

### network.protocol
### destination.port

Both tell you how the attacker is communicating

Is it web traffic? DNS? Something else?
Is it hiding inside normal-looking ports

Building the narrative

<img width="344" height="35" alt="image" src="https://github.com/user-attachments/assets/f9be63b7-8efd-4634-a5da-213f200d66f2" />

### In Kibana > Discover

Make sure index is something broad like so-* or logs-*
Also adjusted the time
<img width="3131" height="1084" alt="image" src="https://github.com/user-attachments/assets/67439420-d638-471d-944d-7f38c22c410d" />

use a simple pivot filter for our alerts

event.category:network

Add internal host ip filter we've already identified.
<img width="3152" height="1067" alt="image" src="https://github.com/user-attachments/assets/51bce572-ab06-4e5c-96a9-fc14c635152c" />


Now in the left field list, search for:
http.request.method > http.method
<img width="387" height="346" alt="image" src="https://github.com/user-attachments/assets/4cd88dd5-0004-48c0-a334-fe3dfde5dbf2" />

url.path > http.uri
<img width="3152" height="1046" alt="image" src="https://github.com/user-attachments/assets/cb14d384-2a45-42c4-9789-a5cef7082f6e" />

network.protocol
<img width="393" height="464" alt="image" src="https://github.com/user-attachments/assets/bf806a02-e9a8-441e-b503-06c9ca9b5cd7" />

Add them to the table if they appear

Now we see a suspicious http.uri
<img width="3157" height="1084" alt="image" src="https://github.com/user-attachments/assets/efcdfd03-8061-47da-bf08-c24a97348f58" />

### Still staying focused on the behavior pattern (NOT the IOC yet)

So far we've:
Isolated the infected host
Found the HTTP activity
Confirmed POST request with repeated URI

Now we want to understand HOW the host is talking - not where

### To our current filter, add:

### AND event.dataset:zeek.http

<img width="3148" height="1080" alt="image" src="https://github.com/user-attachments/assets/f9a48166-d3ce-45f3-bdf8-c0816f0019de" />

### Sort by @timestamp ascending

"As we scroll down, time moves forward"
<img width="606" height="572" alt="image" src="https://github.com/user-attachments/assets/42302d77-3343-4358-bcd2-c966d5b2cb8e" />

### * QUESTION

I had the question of why we want to add event.dataset:zeek.http to the KQL query up top and not add it as a column to the table

ChatGPT says it's the difference between FILTERING vs DISPLAYING

#### Adding to the KQL Query = filtering the data
"Only show me the HTTP logs - hide everything else"

Now our table contains only the actual web request events, which makes patterns much easier to see.

KQL filter = narrowing the investigation scope

#### Adding as a column = just labeling rows
If we add event.dataset as a column:

We still see zeek.http
But we also see zeek.conn, zeek.file, etc

Nothing is removed, we're just tagging each row

We're filtering now so we can understand the behavior pattern.

### Now we're looking at actual web requests
Still reading the behaviors, not IOC

<img width="563" height="430" alt="image" src="https://github.com/user-attachments/assets/52744191-8ee9-48bc-99b1-d1ba2971b2d3" />

Based of the timestamps it looks like consistent, steady traffic involving the /foots.php and follwoing on every second or couple of seconds.

Thinking automated behavior, not human browsing

<img width="745" height="750" alt="image" src="https://github.com/user-attachments/assets/066f9024-c31c-4979-b0cc-9b67b6a76643" />

<img width="761" height="698" alt="image" src="https://github.com/user-attachments/assets/33f6ff22-656e-445e-b250-ae0d75e67926" />

### Now we want to confirm in the timeline graph and change Auto interval to a smaller bucket if available (minute or similar)

<img width="2804" height="823" alt="image" src="https://github.com/user-attachments/assets/9e7fa5ea-4953-4910-b716-b0a1ed577c5d" />

Click and drag mouse over green bars to zoom in

or

Modify the time and date in top right corner

<img width="3132" height="480" alt="image" src="https://github.com/user-attachments/assets/dfbc54d3-89ae-4816-90e7-85807cb0720f" />
Now we can see a clearer picture of POST requests being evenly spaced

We've now identified:
A repeating POSt pattern
Same endpoint
Regular timing in the histogram

### Next: Isolate only the repeating behavior

Add http.method:POST to our KQL search

<img width="2746" height="383" alt="image" src="https://github.com/user-attachments/assets/5d26a02a-7cd8-4869-9c40-cb459e8afb07" />

Now our histogram looks a little cleaner

The timestamped feel machine-paced so we're starting to see behavior instead of just data.

#### Now we can pivot slightly as we see in the http.uri column and scan the POST rows that the POST request always goes to the exact same path

Now we're going to add a column for destination.ip and see if the destination stays the same

<img width="437" height="350" alt="image" src="https://github.com/user-attachments/assets/7faca88c-8d8f-40c0-9977-52280ce62619" />

<img width="2647" height="1033" alt="image" src="https://github.com/user-attachments/assets/47b4d5c2-949a-4bb0-88af-b09248784b91" />

We see the destination.ip address stays constant

<img width="705" height="249" alt="image" src="https://github.com/user-attachments/assets/a3a6d398-c243-4114-aa0e-478ae9e04827" />

Now ChatGPT wants us to add a a source port

<img width="2093" height="791" alt="image" src="https://github.com/user-attachments/assets/25c68523-c4e7-4d8b-b107-6d6af70c147c" />

What changing ports usually mean:

When a machine talks outbound:
The OS picks a random high source port
Each new connection often gets a new port

So seeing:
Same destination
Same URI
Same POST
different source.port

Ususally means new connections being opened repeatedly, not one long browsing session.

Humans don't manually create new TCP sessions every few seconds, software does.

#### Now ChatGPT wants us to add another column for http.response.body.length

<img width="3096" height="1049" alt="image" src="https://github.com/user-attachments/assets/7b5f43d7-c03a-4cbd-ab13-a72e2814949e" />

Responses look similar in size (except for uri of /index.php)

<img width="761" height="171" alt="image" src="https://github.com/user-attachments/assets/b2af3d45-b14c-4a56-8b6f-439249878454" />

<img width="720" height="263" alt="image" src="https://github.com/user-attachments/assets/6e71a7a3-7b52-4723-b3b3-c73b2a023261" />

### We could next add http.status.code OR http.request.body.length

http.status.code

<img width="3128" height="1026" alt="image" src="https://github.com/user-attachments/assets/e2abbc3d-e9bb-4f58-b205-4c4aff30a953" />

All say 200

<img width="679" height="196" alt="image" src="https://github.com/user-attachments/assets/e5425f41-b3b4-4acf-93d0-39485865406e" />

http.request.body

<img width="3161" height="1056" alt="image" src="https://github.com/user-attachments/assets/ec060677-c7c4-4789-a3fc-734f7cfe3e54" />

Now we've proven:

POST requests are happening repeatedly
Same URI
Same destination
Machine-paced timing
Non-zero request body
Server responds 200 with tiny/empty responses

This pattern strongly suggests the client is sending data outbound, not just browsing
We don't need payloads or decoded content, the metadata alone tells us this

Now we pibot from POST requests to GET requests

In our KQL filter, change http.method:POST to http.method:GET

<img width="1614" height="159" alt="image" src="https://github.com/user-attachments/assets/767066e4-f62d-4f32-b442-95f62c73b703" />

These look more like normal web traffic.

Next we're going to switch back to POST view

Are all POSTs going to that repeating endpoint, or are there multiple POST destinations/URIs mixed in?

We then went to Field Statistics for http.uri
<img width="1045" height="177" alt="image" src="https://github.com/user-attachments/assets/b9dfa153-e74e-4f03-9b5d-77971e3ed660" />

We see /foots.php is the dominant POST traffic

## Now we shift from Pattern > Context

We removed "AND http.method:POST"

Now we want to see what happens right before or around the repreating POST behavior

Look at the timeline and the URI's near the first POST event we saw

Do we see any different 

<img width="3135" height="1074" alt="image" src="https://github.com/user-attachments/assets/49bc0efe-9dad-4908-9106-20c56c2a16e2" />

<img width="538" height="363" alt="image" src="https://github.com/user-attachments/assets/01d80dee-85fa-4892-9ba3-9563ef06dbb3" />

<img width="701" height="378" alt="image" src="https://github.com/user-attachments/assets/59c73b4a-3085-4dea-b344-5b002cf048f1" />

<img width="2785" height="229" alt="image" src="https://github.com/user-attachments/assets/a162aed1-7f74-4bc0-a0e9-8ebd0eb855b1" />

<img width="709" height="404" alt="image" src="https://github.com/user-attachments/assets/c566be95-2dbe-4565-80d4-b86aeb6ec936" />

No it does not.

### Next, keep our POST-only filter and add these columns if they exist.

http.user_agent
http.request.body.length
source.port

Then check whether the /foots.php POSTs to 79.124.78.197 have a consistent user-agent + regular timing + changing source port. 
That combo is a strong automated beacon/check-in pattern

<img width="3149" height="1078" alt="image" src="https://github.com/user-attachments/assets/76f41484-bb50-424b-b8c6-74fca3132638" />

<img width="657" height="93" alt="image" src="https://github.com/user-attachments/assets/32d06fe3-8735-4c1b-ba78-5a01bea1bd35" />





































































































